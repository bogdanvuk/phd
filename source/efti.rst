.. include:: conf.rst

.. role:: python(code)
    :language: python

.. highlight:: python

|algo| algorithm
================

This section describes an evolutionary algorithm for oblique full DT induction using supervised learning - |algo|. The main motivation for creating |algo|, was to develop an algorithm that is:

- Suitable for the implementation on embedded systems, i.e. has low hardware resource requirements,
- Easy parallelizable and accelerated in hardware, and
- Induces smaller DTs than the existing solutions, without the loss in DT accuracy.

For all these reasons, it was chosen to base the |algo| algorithm on (1+1) Evolutionary Strategy, since it does not use the population of individuals, as most of existing EA-based DT induction algorithms do.

The algorithm overview
----------------------

The :num:`Algorithm #fig-algorithm-pca` shows the algorithmic framework for the |algo| algorithm, which is similar for all evolutionary algorithms and comprises main tasks of the individual mutation, fitness evaluation and selection, but lacks the crossover step since it does not employ a population of individuals. The DT is induced from the training set - the argument ``train_set`` received by the ``efti()`` function. Since the |algo| algorithm performs supervised learning, the training set consists of problem instances which have known class memberships. The |algo| algorithm maintaines a single candidate solution, stored in the variable ``dt`` in the pseudo-code. The evolution is started from the randomly generated one-node DT by the ``initialize()`` function (containing only the root) and the effort is made iteratively to improve on it. In each iteration, the DT is slightly changed by the ``mutate()`` function to obtain the mutated individual stored into the ``dt_mut`` variable. Two types of mutations are employed on the DT individual:

- Every iteration, a small number of randomly selected coefficients in the certain number of randomly selected nodes are changed, and
- Every few iterations, a node is either added or removed from the DT

.. _fig-algorithm-pca:

.. literalinclude:: code/algorithm.py
    :caption: Overview of the |algo| algorithm

The fitness of the mutated individual (variable ``fit_mut``), calculated by the ``fitness_eval()`` function, is then compared with the fitness of the candidate solution individual (variable ``fit``) by the ``selection()`` function, which decides whether the mutated individual will be taken as the new candidate solution, i.e. will it become the base for the mutation in the iterations to follow. During ``max_iter`` iterations the |algo| algorithm tries to improve upon the DT candidate solution, after which the algorithm exits and the candidate solution is returned. Once the DT is formed in this way, it can be used to classify new instances of the problem.

In the :num:`Figures #fig-efti-overview00` through :num:`#fig-efti-overview07`, one example evolution of the DT by the |algo| algorithm on the ``vene`` dataset is shown. Eight specific moments in the DT evolution, where significant breakthroughs in the fitness of the DT were made, are presented in these figures by both plotting their tree structure, and displaying the partition of the attribute space that these individuals induce. The nodes are drawn using circles and the leaves using squares, and each node and each leaf is assigned a unique ID. Each leaf node and its corresponding attribute space region are labeled in the format *i-Cj*, where *i* equals the ID of the leaf, and *j* equals the class number assigned to the leaf, hence also to the region. For each of these figures, the following information is given:

- Iteration - the iteration number in which the DT individual was evolved
- Fitness - the fitness of the DT
- Size - the size of the DT: calculated as the number of leaves in the DT
- Accuracy - the accuracy of the DT on the training set: calculated as the percentage of the instances from the training set that are classified correctly

.. subfigstart::

.. _fig-efti-overview-dot00:

.. figure:: images/efti_overview_dts/dot00.png
    :width: 100%
    :align: center

    Initial one-node DT generated by the ``initialize()`` function

.. _fig-efti-overview-attr00:

.. figure:: images/efti_overview_dts/dt00.pdf
    :width: 93%
    :align: center

    Initial attribute space partition

.. subfigend::
    :width: 0.48
    :label: fig-efti-overview00

    Iteration: 000000, Fitness: 0.602, Size: 2, Accuracy: 0.600

.. subfigstart::

.. _fig-efti-overview-dot01:

.. figure:: images/efti_overview_dts/dot01.png
    :width: 100%
    :align: center

    No added nodes that were tried managed to increase fitness

.. _fig-efti-overview-attr01:

.. figure:: images/efti_overview_dts/dt01.pdf
    :width: 93%
    :align: center

    Position of the split shifted to increase the accuracy

.. subfigend::
    :width: 0.48
    :label: fig-efti-overview01

    Iteration: 000013, Fitness: 0.629, Size: 2, Accuracy: 0.627

.. subfigstart::

.. _fig-efti-overview-dot02:

.. figure:: images/efti_overview_dts/dot02.png
    :width: 100%
    :align: center

    Three new nodes added to increase the accuracy

.. _fig-efti-overview-attr02:

.. figure:: images/efti_overview_dts/dt02.pdf
    :width: 93%
    :align: center

    Three new splits added for finer attribute space partition

.. subfigend::
    :width: 0.48
    :label: fig-efti-overview02

    Iteration: 003599, Fitness: 0.914, Size: 5, Accuracy: 0.920

.. subfigstart::

.. _fig-efti-overview-dot03:
.. figure:: images/efti_overview_dts/dot03.png
    :width: 100%
    :align: center

    Since the region of leaf #6 contained almost no individuals in the :num:`Figure #fig-efti-overview-attr02`, it was removed and the node #7 was basically moved up to replace node #3 (:num:`Figure #fig-efti-overview-dot02`), and thus removing the said empty region.

.. _fig-efti-overview-attr03:
.. figure:: images/efti_overview_dts/dt03.pdf
    :width: 93%
    :align: center

    The region of the leaf #6 (:num:`Figure #fig-efti-overview-attr02`) was removed, since it was almost empty and contributed little to accuracy. The resulting DT is smaller, even with a slight increase in accuracy (since the split induced by node 1 has also shifted slightly to a better position).

.. subfigend::
    :width: 0.48
    :label: fig-efti-overview03

    Iteration: 007859, Fitness: 0.927, Size: 4, Accuracy: 0.930

.. subfigstart::

.. _fig-efti-overview-dot04:
.. figure:: images/efti_overview_dts/dot04.png
    :width: 100%
    :align: center

    The leaf #5 was made into a node

.. _fig-efti-overview-attr04:
.. figure:: images/efti_overview_dts/dt04.pdf
    :width: 93%
    :align: center

    Small increase in accuracy was obtained by further dividing the central region of the attribute space, where the individuals of all three classes overlap

.. subfigend::
    :width: 0.48
    :label: fig-efti-overview04

    Iteration: 030268, Fitness: 0.927, Size: 5, Accuracy: 0.933

.. subfigstart::

.. _fig-efti-overview-dot05:
.. figure:: images/efti_overview_dts/dot05.png
    :width: 100%
    :align: center

    The leaf #4 was now made into a node

.. _fig-efti-overview-attr05:
.. figure:: images/efti_overview_dts/dt05.pdf
    :width: 93%
    :align: center

    Again, further division of cental attribute space region produced a small increase in accuracy. Fitness has progressed even less, since the addition of a new node diminished the advantage of a small accuracy increase.

.. subfigend::
    :width: 0.48
    :label: fig-efti-overview05

    Iteration: 177050, Fitness: 0.927, Size: 6, Accuracy: 0.937

.. subfigstart::

.. _fig-efti-overview-dot06:
.. figure:: images/efti_overview_dts/dot06.png
    :width: 100%
    :align: center

    The leaf #8 was split into two

.. _fig-efti-overview-attr06:
.. figure:: images/efti_overview_dts/dt06.pdf
    :width: 93%
    :align: center

    The region of leaf #8 was split, bringing no improvement to the class separation, but with some other shifts in the split positions, some small accuracy gain was achieved

.. subfigend::
    :width: 0.48
    :label: fig-efti-overview06

    Iteration: 279512, Fitness: 0.927, Size: 7, Accuracy: 0.940

.. subfigstart::

.. _fig-efti-overview-dot07:
.. figure:: images/efti_overview_dts/dot07.png
    :width: 100%
    :align: center

    Leaf #9 was removed together with the node #4, which brought the node #8 up in the place of the node #4. Leaves #10 and #11 were removed, and the node #5 was reverted to leaf again.

.. _fig-efti-overview-attr07:
.. figure:: images/efti_overview_dts/dt07.pdf
    :width: 93%
    :align: center

    |algo| gave up on finely partitioning the central attribute space region, since very little gain in accuracy could not justify the increase in the DT size, and it managed to produce the smaller DT without sacrificing the accuracy. The split by the node #8 between the regions #12 and #13 in the :num:`Figure #fig-efti-overview06`, became the split between the regions #8 and #9 after the node #8 moved up to replace the node #4. This, once useless split, has now shifted to turn out very usefull in separating instances of the classes :math:`C_1` and :math:`C_3` and hence contributing to the accuracy.

.. subfigend::
    :width: 0.48
    :label: fig-efti-overview07

    Iteration: 415517, Fitness: 0.934, Size: 5, Accuracy: 0.940

At the beggining of the |algo| algorithm, the initial individual needs to be generated (:num:`Figure #fig-efti-overview00`). Since |algo| has a goal of creating DTs as small as possible, the initial individual will be first created empty and than only the root node will be generated and inserted into it. By the iteration #13 (:num:`Figure #fig-efti-overview01`), no new nodes were added, but the root node test was modified to produce the increase in the DT accuracy from 0.6 to 0.627. During the further evolution the nodes are added, which raises the accuracy of the DT. Notice how fitness starts to deviate from the accuracy when more nodes are added. This is because the fitness also depends on the size of the DT, in that it is penalized the more leaves the DT has. In this example, the biggest drop in the fitness because of the DT size is in the iteration #279512 of the DT evolution (:num:`Figure #fig-efti-overview06`), where the DT individual comprises 7 leaves and although the accuracy climbs to 0.94 (classification success rate of 94%), the fitness remains at 0.927. In this way, the evolutionary process is forced to search for the smaller DT solutions, in which it eventually succeds by the iteration 415517 (:num:`Figure #fig-efti-overview07`), where the DT size drops to only 5 leaves without affecting the accuracy.

Detailed description
--------------------

In this section, the detailed descriptons of individual |algo| subtasks is given. Although |algo| was based on the (1+1)-ES, there are many additional features that need to be implemented in order for it to have high performance for DT induction, like tree structure mutation procedure, fitness calculation specifics, etc.

.. _sec-mutation:

Mutation
........

The |algo| algorithm performs two types of mutations on the DT individual:

- The node test coefficients mutation
- The DT topology mutation

During each iteration of the |algo| algorithm, a small portion (|alpha|) of DT nodes' test coefficients is mutated at random. When the node test coefficients mutation is performed, the selected coefficient is mutated by changing its value by a random small number. Every change in the node test influences the classification, as the instances take different paths through the DT, hence being classified differently. Usually, only one coefficient per several nodes (dictated by the parameter (|alpha|) is mutated in each iteration, in order for the classification result to change in small steps.

.. subfigstart::

.. _fig-node-addition-before:

.. bdp:: images/node_addition_before.py
    :width: 80%
    :align: center

    DT before addition of the node in place of the leaf #2

.. _fig-node-addition-after:

.. bdp:: images/node_addition_after.py
    :width: 80%
    :align: center

    DT after a node has been added in place of the leaf #2

.. subfigend::
    :width: 0.35
    :label: fig-node-addition

    Example showing how a DT is mutated by adding a node to it

.. subfigstart::

.. _fig-node-removal-before:

.. bdp:: images/node_removal_before.py
    :width: 90%
    :align: center

    DT before addition of the node in place of the leaf #2

.. _fig-node-removal-after:

.. bdp:: images/node_removal_after.py
    :width: 90%
    :align: center

    DT after a node has been added in place of the leaf #2

.. subfigend::
    :width: 0.40
    :label: fig-node-removal

    Example showing how a DT is mutated by adding a node to it

On the other hand, the topology mutations represent very large moves in the search space, so they are performed even less often. In every iteration, there is a small chance (|rho|) that a single node will either be added to the DT or removed from it. This change either adds an additional test for the classification, or removes one or whole subtree of tests. The node is always added in place of an existing leaf, i.e. never in place of an internal non-leaf node, as shown in the example in the :num:`Figure #fig-node-addition`. The test coefficients of the newly added non-leaf node are calculated in the same way as are the root test coefficients during the DT initialization, using the procedure explained in the :num:`Section #sec-node-insertion`. On the other hand, if a node is to be removed, first a leaf is selected at random. Then both the leaf and its parent are removed from the DT, while the leaf's sibling moves up to replace its former parent, as shown in the example in the :num:`Figure #fig-node-removal`. By adding a test, a new point is created, where during the classification, instances from different classes might separate and take different paths through the DT and eventually be classified as different, which can in turn increase the accuracy of the DT. On the other hand, by removing unnecessary tests, the DT is made smaller. The size of the DT is also an important factor in the fitness calculation in the |algo| algorithm as discussed in the :num:`Section #sec-oversize`.

.. _sec-node-insertion:

The DT node insertion algorithm
...............................

Each time a node is to be added to the DT, whether it is the root node in the initialization procedure or any other node in the mutation procedure, the node's test needs to be initialized. Initializing the test coefficients with random numbers proved to be an impediment to the evolution process, since there is a rather small probability for a node test generated in this way to provide a usefull split in the attribute space, i.e. a split that divides instances of different classes. With this procedure, the hyperplane usually lands completely outside the attribute space region where the instances are located, where the :num:`Figure #fig-split-init-miss1` shows one such hyperplane as an example. Even if the hyperplane intersects the area of the attribute space where the instances reside, the split can still be ineffective in the way that it does not help distinguish instances of different classes, i.e. it does not contribute to the DT accuracy, where the :num:`Figure #fig-split-init-miss2` shows one such hyperplane as an example. This influences the algorithm convergence negatively, in that it takes too many generations to relocate the ill-positioned hyperplane to the location where it starts contributing to the accuracy of the DT individual.

.. subfigstart::

.. _fig-split-init-miss1:

.. plot:: images/split_init_miss1.py
    :width: 100%
    :align: center

    Hyperplane initialized to the position outside the region where the instances reside

.. _fig-split-init-miss2:

.. plot:: images/split_init_miss2.py
    :width: 100%
    :align: center

    Hyperplane initialized to the position where it does not contribute to the DT accuracy

.. subfigend::
    :width: 0.48
    :label: fig-split-init-miss

    Hyperplanes cannot be initialized completely at random, since there is a high chance of them being ineffective

However, in order to allow for wider search space exploration, the node tests need to be generated at random, but this process needs to be guided by the structure of the training set to speed up the convergence of the evolutianary algorithm towards the optimal solution. One of the approaches for the random initialization basically ensures that two randomly selected training set instances (called a mixed dipole) take different paths during classification at the node being initialized, and is suggested in :cite:`krketowski2005global`. The mixed dipole comprises two instances from the training set that belong to different classes. As shown in the :num:`Figure #fig-dipole-hyperplane`, the procedure consists of placing the hyperplane :math:`H_{ij}(\mathbf{w},\theta)` in the attribute space, perpendicular to the line connecting the mixed dipole :math:`(\mathbf{x}^i, \mathbf{x}^j)`. The hyperplane corresponds to the node test given by the equation :eq:`oblique-test`, where |w| is the test coefficient vector and |th| is the test threshold. The attribute space of the ``vene`` dataset, used in this example has two dimensions, one for each of the attributes :math:`x_1` and :math:`x_2`. The hyperplane's exact position is further fixed by randomly generated parameter :math:`\delta \in (0,1)`, which determines whether the hyperplane is placed closer to :math:`\mathbf{x}^i` (for :math:`\delta < 0.5`), or closer to :math:`\mathbf{x}^j` (for :math:`\delta > 0.5`). Mathematically, the equation for the hyperplane generated by the method of the mixed dipole described in this paragraph is obtained in the following way:

.. math::
    :label: eq-rnd-dipole-hyperplane

    H_{ij}(\mathbf{w},\theta) &= \mathbf{w}\begin{pmatrix}x_1\\x_2\end{pmatrix} - \theta,\\
    \mathbf{w} &= (\mathbf{x}^i - \mathbf{x}^j),\\
    \theta &= \delta\mathbf{w}\cdot\mathbf{x}^i + (1-\delta)\mathbf{w}\cdot\mathbf{x}^j

.. _fig-dipole-hyperplane:
.. plot:: images/dipole_hyperplane_plot.py
    :width: 70%

    Initialization of the node test based on the randomly chosen dipole :math:`H_{ij}(\mathbf{w},\theta)` is a hyperplane corresponding to the node test, |w| is coefficient vector, and |th| is the threshold. The attribute space of the ``vene`` dataset used in this example has two dimensions, one for each of the attributes :math:`x_1` and :math:`x_2`.

This procedure aims to introduce a usefull test into the DT, based on the assumption that the instances of the same class are somehow grouped in the attribute space, and that the test produced in this way will help separate the instances belonging to the instances of the dipole.

Fitness evaluation
..................

The DT can be optimized with respect to various parameters, where the DT accuracy and its size are usually the most important. However, there are many more parameters of interest, like the number of training set classes not represented in the DT, the purity of the DT leaves, the deegree at which the DT is balanced, etc. Hence, in order to solve this multi-objective optimizational problem with the evolutionary approach, a fitness function needs to be defined to effectively collapse it to a single objective optimizational problem. This can be done in various ways, and here one procedure, employed in the |algo| algorithm is given.

.. _fig-fitness-eval-pca:

.. literalinclude:: code/fitness_eval.py
    :caption: The pseudo-code of the fitness evaluation task.


Accuracy calculation
;;;;;;;;;;;;;;;;;;;;

The main task of the optimization procedure is to maximize the accuracy of the DT individual on the training set. The accuracy is calculated by letting the DT individual classify all problem instances from the training set and then by comparing the classification results to the desired classifications, specified in the training set. The pseudo-code for this task is given in the :num:`Algorithm #fig-accuracy-calc-pca`. The input parameter ``dt`` is the current DT individual and ``train_set`` is the training set.

.. _fig-accuracy-calc-pca:

.. literalinclude:: code/accuracy_calc.py
    :language: python3
    :caption: The pseudo-code of the accuracy calculation task.

First, the class distribution is determined by letting all instances from the training set traverse the DT, i.e. by calling the ``find_dt_leaf_for_inst()`` function whose pseudo-code is given in the :num:`Algorithm #fig-find-dt-leaf-for-inst-pca`. This function determines the instance traversal path, and returns the leaf node in which the instance finished the traversal. The traversal starts at the root node (accessed via ``dt.root``), and is performed in the manner depicted in the :numref:`fig-dt-traversal`, where one possible path is given by the curvy line. Until a leaf is reached, the node tests are performed and the decisions to which child to proceed are made based on the test outcomes. The function ``dot_product()``, calculates the scalar product of the node test coefficient vector |w| (stored in ``cur_node.w`` attribute), and the attribute vector of the instance |x| (stored in ``instance.x`` variable), and the value returned is compared with the node test threshold |th| (stored in ``cur_node.thr`` attribute).

.. _fig-find-dt-leaf-for-inst-pca:

.. literalinclude:: code/find_dt_leaf_for_inst.py
    :caption: The pseudo-code of the procedure for determining the end-leaf for an instance.

Next step in the accuracy calculation process (the first for loop in the :num:`Algorithm #fig-accuracy-calc-pca`) is to calculate the class distribution matrix. The distribution matrix, shown in the :num:`Figure #fig-distribution-matrix`, has one row for each DT leaf, i.e. for each attribute space partition induced by the DT. Each row in turn contains one element for each of the classes in the training set. Hence, a row of the distribution matrix contains the statistics on how many instances of each of the training set classes finished the traversal in the leaf corresponding to the row.

.. _fig-distribution-matrix:

.. bdp:: images/distribution_matrix.py
    :width: 80%

    The structure of the distribution matrix. From for each matrix row *i*, we obtain the dominant class :math:`k_i` and the number of instances of the dominant class :math:`d_{(i,k_i)}` that finished the traversal in the leaf with ID *i*.

The classes of all the instances from the training set are known and accessed for each instance via the attribute ``instance.cls`` (within the ``accuracy_calc()`` function). For each instance in the training set, based on the ID of the leaf in which it finished the traversal (attribute ``leaf.id``, where the ``leaf`` is returned by the ``find_dt_leaf_for_inst()`` function) and the instances class, the distribution matrix is updated. The :math:`d_{i,j}` element of the distribution matrix contains the number of instances of the class *j* (:math:`C_j`) that finished in the leaf node with the ID *i* after the DT traversal. After all the instances from the training set traverse the DT, this matrix contains the distribution of classes among the leaf nodes.

The second ``for`` loop of the ``accuracy_calc()`` function finds the dominant class for each leaf node. The dominant class for a leaf node is the class having the largest percentage of instances, among the ones that finished the traversal in that leaf node. Formally, the dominant class :math:`k_i` of the leaf node with the ID *i* is:

.. math:: k_i | (d_{(i,k_i)} = \max_{j}(d_{i,j}))
    :label: dominant_class

The structure of the distribution matrix is displayed in the :num:`Figure #fig-distribution-matrix`. Rows correspond to the leaves of the DT, and the columns correspond to the classes of the training set. From the distribution matrix we obtain for each row *i*, the dominant class :math:`k_i` and the number of instances of the dominant class :math:`d_{(i,k_i)}` that finished the traversal in the leaf with ID *i*.

If we were to do a classification run with the current DT individual over the training set, the maximum accuracy would be attained if all leaf nodes were assigned their corresponding dominant classes. Thus, each instance which finishes in a certain leaf node, that belongs to that node's dominant class, is added to the number of classification hits (the ``hits`` variable of the :num:`Algorithm #fig-accuracy-calc-pca`), otherwise it is qualified as a missclassification. Hence,

.. math:: hits=\sum_{i=1}^{N_l}{d_{(i,k_i)}}.
    :label: hits_sum

The accuracy of the DT is hence the percentage of the instances whose classifications were declared as hits, as given in the pseudo-code: ``accuracy = hits / len(train_set)``.

.. _sec-oversize:

Oversize
;;;;;;;;

The DT oversize is calculated as the relative difference between the number of leaves in the DT and the total number of classes (|Nc|) in the training set (obtained via the ``train_set.cls_cnt()`` function). In order to be able to classify correctly all training set instances, after the DT induction, the DT needs to have at least one leaf for each class occurring in the training set. Therefore, without knowing anything else about the dataset, our best guess is that the minimal DT that could be consistent with the dataset has one leaf for each of the dataset classes. For that reason, the oversize measure, given by the equation :eq:`eq-oversize`, was defined in such a way to have the DT start suffering penalties to the fitness when the number of the DT leaves exceeds the total number of classes in the training set, i.e. the oversize measure is zero when :math:`\Nl=\Nc`.

.. math:: oversize = \frac{\Nl - \Nc}{\Nc}
    :label: eq-oversize

DT oversize negatively influences the fitness as it can be seen from the way fitness is calculated in the :num:`Algorithm #fig-fitness-eval-pca`: ``fitness = accuracy * (1 - Ko*oversize*oversize)``. The parameter |Ko| is used to control how much influence the DT oversize will have on the overall fitness. In other words, it determines the shape of the collection of Pareto frontiers for the DT individual. Each DT individual can be represented as a point in a 2-D space induced by the DT oversize and accuracy measures. A Pareto set is formed for each possible fitness value, where all elements of the set are assigned the same fitness value, even though they have different accuracy and oversize measures.

.. _fig-fit-overSize:
.. plot:: images/pareto.py
    :width: 70%

    Position of Pareto frontiers for accuracy value of 0.8, when |Nc| equals 5, for |Ko| parameter values of: 0, 0.02 and 0.1.

The :num:`Figure #fig-fit-oversize` shows the position of the Pareto frontier for an example of fitness value of 0.8 and few values of the parameter |Ko|. Also, for this example, it was taken for |Nc| to equal 5. It can be seen that if |Ko| is chosen to be 0, the oversize does not influence the fitness, which is in turn always equal to the value of the accuracy. When :math:`K_o > 0`, the |algo| algorithm will be willing to trade accuracy for the DT size. As it can be seen from the figure, when for an example the parameter |Ko| has the large value of 0.1, big DTs are highly discouraged in that an individual of size 5 with the accuracy of 0.8 is equally fit in the eyes of the algorithm as the one of size 10 with even more than 10% higher accuracy.

As shown in the :num:`Algorithm #fig-fitness-eval-pca`, the dependence of the fitness on the oversize measure is quadratic. This serves two purposes:

#. Since oversize turns negative when the DT size falls below |Nc|, such undersized DTs would be getting a boost in fitness if it were not for the squaring. If all classes are to be represented in the DT, the number of leaves should at least match the number of classes, when it would be at least possible for each class to have a leaf. By squaring the oversize, the undersized DTs are discouraged in the same way the oversized are.

#. By using the quadratic dependence, the rate at which fitness decreases with the DT size is lower when the size is closer to the |Nc|, and gets progressively higher as the size increases. This way, the DTs whose size is close to |Nc| are penalized less then they would be if the dependence of the fitness on oversize were linear.

In order to measure the influence of the oversize on the induced DTs, an experiment has been conducted on all datasets from the :numref:`tbl-uci`. The DTs were induced for a number of values for the parameter |Ko|, namely :math:`K_o \in \{0, 0.001, 0.01, 0.02, 0.06, 0.1, 0.2\}`. The results are presented in the Tables :num:`#tbl-oversize-size-comp` and :num:`#tbl-oversize-acc-comp`, and Figures :num:`#fig-oversize-comp1` and :num:`#fig-oversize-comp2`. :num:`Table #tbl-oversize-size-comp` lists the induced DT sizes and :num:`Table #tbl-oversize-acc-comp` lists the induced DT accuracies for all values of the oversized weight parameter |Ko| used in the experiment. The values in the :num:`Table #tbl-oversize-size-comp` clearly indicate that the largest DTs are induced when the DT oversize is ignored during induction, :math:`K_o=0`. From there, the induced DT sizes drop quickly when the value of |Ko| is increased, only to start saturating after certain |Ko| value, which is different for each dataset. This is usually the place where the |algo| algorithm needs to start inflicting serious damage to the DT accuracies, only to compress them furher in size by small factors. This trend can be also observed with accuracies in the :num:`Table #tbl-oversize-acc-comp`. The accuracies are, naturaly, largest when there is no size limit imposed, i.e. :math:`K_o=0`. Then, as the value of |Ko| increases, the induced DTs of some of the datasets experience a significant drop in the accuracy, where this drop is of course traded-off against a significant drop in their sizes. These datasets, like bch, cmc, krkopt, letter, ttt, wfr, wine, etc., are the ones whose internal complexity really demands for bigger DTs in order to describe them more precisely. On the other hand, the induced DTs of some of the datasets, experience little or no change in the accuracy when the |Ko| value increases up to a certain point. For these datasets, like ausc, bank, bcw, irs, psd, shuttle, sick, zoo, etc., initial large DTs are indeed excessive in size and the more succint DT representation was succesfully found by the |algo| aglorithm. When the |algo| algorithm is used in practice, it is a design choice whether the most accurate DTs are needed no mather their size, or we are interested in the smallest DTs at the cost of their accuracy, or we are willing to accept certain trade-off between the DT size and its accuracy. It is obvious from these results that there is a different behavoiur of the inferred DTs from different datasets, in terms of DT accuracies and sizes, when the oversize fitness weight |Ko| is varied. Hence, the actual value of the |Ko| parameter will depend on the domain of the problem being solved.

.. raw:: latex

   \begingroup
   \small
   \renewcommand{\arraystretch}{0.8}

.. tabularcolumns:: L{0.15\linewidth} | R{0.08\linewidth} R{0.08\linewidth} R{0.08\linewidth} R{0.08\linewidth} R{0.08\linewidth} R{0.08\linewidth} R{0.08\linewidth}

.. _tbl-oversize-size-comp:
.. csv-table:: List of datasets (and their characteristics) from the UCI database, that are used in the experiments throughout this thesis
    :header-rows: 1
    :file: scripts/oversize-comp-size.csv

.. tabularcolumns:: L{0.15\linewidth} | R{0.08\linewidth} R{0.08\linewidth} R{0.08\linewidth} R{0.08\linewidth} R{0.08\linewidth} R{0.08\linewidth} R{0.08\linewidth}

.. _tbl-oversize-acc-comp:
.. csv-table:: List of datasets (and their characteristics) from the UCI database, that are used in the experiments throughout this thesis
    :header-rows: 1
    :file: scripts/oversize-comp-acc.csv

.. raw:: latex

    \endgroup

.. subfigstart::

.. _fig-oversize-comp-size0:

.. figure:: images/oversize-comp/size0.pdf
    :align: center

    DT size: bc, ion, lym, pen, sb

.. _fig-oversize-comp-acc0:

.. figure:: images/oversize-comp/acc0.pdf
    :align: center

    DT acc: bc, ion, lym, pen, sb

.. _fig-oversize-comp-size1:

.. figure:: images/oversize-comp/size1.pdf
    :align: center

    DT size: cmc, eb, eye, letter, seg

.. _fig-oversize-comp-acc1:

.. figure:: images/oversize-comp/acc1.pdf
    :align: center

    DT acc: cmc, eb, eye, letter, seg

.. _fig-oversize-comp-size2:

.. figure:: images/oversize-comp/size2.pdf
    :align: center

    DT size: ctg, cvf, hrtc, liv, ttt

.. _fig-oversize-comp-acc2:

.. figure:: images/oversize-comp/acc2.pdf
    :align: center

    DT acc: ctg, cvf, hrtc, liv, ttt

.. _fig-oversize-comp-size3:

.. figure:: images/oversize-comp/size3.pdf
    :align: center

    DT size: bch, vene, wfr, wine

.. _fig-oversize-comp-acc3:

.. figure:: images/oversize-comp/acc3.pdf
    :align: center

    DT acc: bch, vene, wfr, wine

.. _fig-oversize-comp-size4:

.. figure:: images/oversize-comp/size4.pdf
    :align: center

    DT size: adult, car, gls, magic, pid

.. _fig-oversize-comp-acc4:

.. figure:: images/oversize-comp/acc4.pdf
    :align: center

    DT acc: adult, car, gls, magic, pid

.. subfigend::
    :width: 0.48
    :label: fig-oversize-comp1

    The figure shows the dependencies of the DT sizes and accuracies on the oversize weight (|Ko|) parameter values. DT sizes and accuracies are displayed for five datasets per subfigure.

.. subfigstart::

.. _fig-oversize-comp-size5:

.. figure:: images/oversize-comp/size5.pdf
    :align: center

    DT size: krkopt, son, w21, w40

.. _fig-oversize-comp-acc5:

.. figure:: images/oversize-comp/acc5.pdf
    :align: center

    DT acc: krkopt, son, w21, w40

.. _fig-oversize-comp-size6:

.. figure:: images/oversize-comp/size6.pdf
    :align: center

    DT size: bcw, ger, irs, mushroom, page

.. _fig-oversize-comp-acc6:

.. figure:: images/oversize-comp/acc6.pdf
    :align: center

    DT acc: bcw, ger, irs, mushroom, page

.. _fig-oversize-comp-size7:

.. figure:: images/oversize-comp/size7.pdf
    :align: center

    DT size: ausc, bank, ca, hep, hrts

.. _fig-oversize-comp-acc7:

.. figure:: images/oversize-comp/acc7.pdf
    :align: center

    DT acc: ausc, bank, ca, hep, hrts

.. _fig-oversize-comp-size8:

.. figure:: images/oversize-comp/size8.pdf
    :align: center

    DT size: nurse, psd, shuttle, sick, spect

.. _fig-oversize-comp-acc8:

.. figure:: images/oversize-comp/acc8.pdf
    :align: center

    DT acc: nurse, psd, shuttle, sick, spect

.. _fig-oversize-comp-size9:

.. figure:: images/oversize-comp/size9.pdf
    :align: center

    DT size: jvow, spf, thy, veh, vote

.. _fig-oversize-comp-acc9:

.. figure:: images/oversize-comp/acc9.pdf
    :align: center

    DT acc: jvow, spf, thy, veh, vote

.. subfigend::
    :width: 0.48
    :label: fig-oversize-comp2

    The figure shows the dependencies of the DT sizes and accuracies on the oversize weight (|Ko|) parameter values. DT sizes and accuracies are displayed for five datasets per subfigure.

Selection
.........

The selection task is responsible for deciding in each iteration which DT will be taken for the candidate solution for the next iteration: either the current candidate solution, i.e. the parent, or the mutated individual. The selection procedure implemented by the :num:`Algorithm #fig-selection-vanilla-pca` is the most basic one, where whenever the mutated individual outperforms its parent in fitness, it is always taken as the new candidate solution, and is discraded otherwise.
An improvement to this basic version of the selection procedure will be discussed in the :num:`Section #sec-search-probability`, where a less fit individual is given a chance under certain circumstances to be selected.

.. _fig-selection-vanilla-pca:
.. literalinclude:: code/selection-vanilla.py
    :caption: The pseudo-code of the :samp:`selection()` function of the |algo| algorithm, that implements the basic individual selection procedure


Possible improvements to the |algo| algorithm
---------------------------------------------

In this section several additional features that can improve either the execution time or the quality of solutions produced by the |algo| algorithm are discussed:

- **Test this!** Fitness dependence on the missing classes - number of classes that are not assigned to any leaf
- Return to the best candidate - give a chance to the evolutionary process to abondon the current candidate solution and to return to the best solution yet.
- **Test this!** Impurity
- **Test this!** Increase in search probability
- **Test this!** Delta classification
- Mersenne twister was used to no avail

In order to test whether |algo| really benefits from a new feature, the fitnesses of the DTs induced by the basic |algo| and |algo| with the feature included, were compared for all UCI datasets listed in the :num:`Table #tbl-uci`. For each dataset, five 5-fold cross-validations were performed. In order to discover whether there is a statistical difference between the fitnesses of the DTs, one-way analysis of variance (ANOVA) :cite:`neter1996applied` has been applied on collected data with the significance level set at 0.05. **When the ANOVA analysis indicated that at least one of the results was statistically different from the others, the Tukey multiple comparisons test :cite:`hochberg2009multiple` was used to group the algorithms into groups of statistically identical results.**

Percentage of missing classes
.............................

The percentage of missing classes is calculated as the percentage of the classes for which the DT does not have a leaf, to the total number of classes in the training set (|Nc|):

.. math:: missing = \frac{\Nc - N_{DTc}}{\Nc}
    :label: eq-missing

where |NDTc| is the number of classes represented in the DT leaves. The fitness calculation is then updated so that the penalties are taken for the missing classes in the DT individual: ``fitness = accuracy * (1 - Ko*oversize*oversize) * (1 - Km*missing)``, where the parameter |Km| is used to control how much influence the number of missing classes will have on overall fitness.

Return to the best candidate
............................



Impurity
........

.. _sec-search-probability:

Search probability
..................

Evolution is inherently an unpredictable process. It is akin to searching for the highest peak in the mountain range, but only being able to see ones immediate vicinity, i.e. not being able to peak at distant mountain tops that could guide ones exploration (see :num:`Figure #fig-escape-local-optimum`). Simplest strategy for conquering the peak closest to ones current location is to always choose the path that leads upwards. This strategy is thus called the greedy hill-climbing strategy. However, there is no guarantee that the closest peak is in the same time the highest in the mountain range and it often is not. One example of such a peak is the peak marked by the letter A in the :num:`Figure #fig-escape-local-optimum`, which is called the local maximum. It is maximum since all points in its neghborhood have lower elevation, but it is only local since there is a higher peak in our search space, namely B from the :num:`Figure #fig-escape-local-optimum`. The greedy approach described above fails in finding a path from point A to point B, since there exist no monotonically uphill path connecting A to B. In order to get to point B the exploration has to first traverse through the regions with lower elevation, shown by an arrow in the :num:`Figure #fig-escape-local-optimum`, in order to get to the base of the hill from which it can start movin up again.

.. _fig-escape-local-optimum:
.. plot:: images/escape_local.py
    :bbox: tight
    :pad: -1.4
    :width: 80%

    An example of the hill climbing problem and the issue of escaping the local optimum A by a greedy strategy in order to reach point B.

The search space for the optimal DT individual has much higher dimensionality and is thus much more complicated than the hill-climbing problem described above. Also, instead of striving for higher elevation, we strive for higher DT individual fitness, and instead of walking in the mountains, we are mutating the DT individual to move around in the search space. However, the main idea is the same, in order to visit as discover as many fitness peaks as possible (in order to find the highest one), we sometimes need to pursue a less fit individual. Since we are unable to tell, which poorer performing solution will eventually lead to an improved one, this decision is made at random with some probability. This probability is called the search probability, since it allows for the evolution process to search the wider neighborhood of the current solution. Without this search, the system would tend to get stuck at local maximas.

**Ovo cak nije ni uradjeno, probati da li unapredjuje rezultate**

This concept is well documented in the Simulated Annealing literature. The test probability starts off with high values and reduces over time. This is referred to in the literature as the cooling schedule. The basic idea is to allow the system a lot of freedom at the beginning of the run when the system is in a high state of disorder in order to allow it to search for optimal structures. Then as structures emerge the freedom is reigned in so that the structures aren’t destroyed. Typically cooling schedules are predefined, although it has been shown that adaptive schedules produce better results.

HereBoy employs an adaptive scheme to reduce the search probability. The search probability is defined by Formula 5 which closely resembles the adaptive mutation rate formula. Again, the output is the product of two terms: the maximum search probability (ρ) and a fractional term that reduces from 1 to 0 as the process converges (β). The maximum search probability is a user-defined parameter between 0 and 1. It defines the maximum chance that a poor performing mutation will be accepted. The fractional term is identical to the one in the adaptive mutation rate formula and performs the same function, to reduce the output from the maximum to 0 as the process converges.

.. subfigstart::

.. _fig-veh-searchprob-fitpath-hereboy:
.. plot:: images/veh_searchprob_fitpath_hereboy.py
    :bbox: tight
    :width: 80%

.. _fig-jvow-searchprob-fitpath-hereboy:
.. plot:: images/jvow_searchprob_fitpath_hereboy.py
    :bbox: tight
    :width: 80%

.. subfigend::
    :width: 1
    :label: fig-searchprob-fitpath-hereboy

    max-iter-comp

.. _fig-adaptive-search-eq:

.. figure:: images/adaptive_search_eq.png
    :width: 60%
    :align: center

    The equations explaining the adaptive search

.. _fig-selection-pca:
.. literalinclude:: code/selection.py
    :caption: The pseudo-code of the :samp:`selection()` function of the |algo| algorithm, that implement's the individual selection procedure

Partial reclassification
........................

As it was already discussed in the :num:`Section #sec-mutation`, the DT mutations alter only a small portion of the DT in each iteration, hence only the classification of the instances on whose traversal paths the mutated nodes happen to reside, will be affected by the mutation. Therefore the majority of instances will travel along identical paths from iteration to iteration, meaning that all related computations will remain the same. Recomputation is thus only necessary for the instances whose paths contain a mutated node. Please also notice that even when the mutated node test coefficients change, only the elements of the vector scalar product sum (given in the equation :eq:`oblique-test`) that correspond to the muatted coefficients must be recomputed, while the computation of all other elements can be skipped.

Therefore, the traversal paths could be memorized for the candidate DT individual in order to avoid unnecessary recalculations of the node tests during the classification of the mutated DT individual, for the instances whose paths do not cross the mutated nodes. Each instance could start the DT traversal by following its memorized path from the candidate DT individual classification, and checking whether it will encounter any of the mutated nodes while traversing the DT. While no mutated nodes are encountered, no test recalculations need to be executed and the instance moves through the DT as dictated by the path stored in memory. When the instance encounters a mutated node, its path in the mutated DT might diverge from its memorized path. If the topological mutation produced the changes in the encountered node, where either a new node was added in the place of a leaf (see :num:`Figure #fig-node-addition` for an example) or the node was removed and a different one took its place (see :num:`Figure #fig-node-removal` for an example), the subtree which the instance has reached has changed, and the rest of the traversal path needs to recomputed. If the node is encountered with only some of its coefficients |w| mutated, the dot product of the mutated node test (:math:`\mathbf{w^{mut}}\cdot \mathbf{x}`), can be calculated based on the dot product of the original node test (:math:`\mathbf{w}\cdot \mathbf{x}`) in the following way:

.. math:: \mathbf{w^{mut}}\cdot \mathbf{x} = \mathbf{w}\cdot \mathbf{x} + \sum_{i \in M}(w^{mut}_{i} - w_{i}) x_{i},
    :label: dot-product-recalc

where *M* is the set of all indices of the mutated coefficients in that node. Furthermore, the mutation on the encountered node may not be strong enough to deflect the instance from its previous path. Hence, the outcome of the mutated node test is monitored whether it will align with the stored path, in which case the instance has not diverged and the instance can continue following the memorized path. Otherwise, the instance entered a new DT subtree and all subsequent node tests need to be recalculated.

In the case the mutated individual is selected for the new candidate solution, the paths which have diverged in the classification run need to be updated to the memory. One possible way to implement this is to keep track of each deviation from the memorized paths during the classification run for the mutated DT individual, and apply all these changes to the stored traversal paths if the individual is selected for the new candidate solution. However, a different method that took advantage of the fact that usually less than 1% of the mutated individuals get selected, proved to be more efficient with respect to both time of execution and the memory resources. With this approach, the |algo| algorithm does not keep track of the deviations from the memorized paths in each classification run of a mutated DT, which in turn saves on memory access time and on the memory space for tracking the changes. Only once a mutated DT has been selected for the new candidate solution is the classification rerun with the instruction to change the stored traversal paths in the memory where needed.

The proposed partial reclassification algorithm has an additional performance issue with the small DT individuals. If the DT individual is only one or two levels deep, there is very large probability that many of the instance paths will be affected by the mutation, and the time consumption overhead of the partial reclassification exceeds its benefits. The |algo| algorithm implements a strategy to turn the partial reclassification off when it operates with small individuals.

.. _fig-partial-find-dt-leaf-for-inst:

.. literalinclude:: code/find_dt_leaf_for_inst_delta.py
    :caption: The modified :samp:`find_dt_leaf_for_inst()` function that implements the partial reclassification method

The pseudo-code in the :num:`Algorithm #fig-partial-find-dt-leaf-for-inst` describes the implementation of the partial reclassification method whithin ``find_dt_leaf_for_inst()`` function (the original implementation is given by the :num:`Algorithm #fig-find-dt-leaf-for-inst-pca`). If the partial classification is turned off by |algo| algorithm (by passing the value ``True`` for the argument ``recalc_all``), the paths of all the training set instances will be imediately considered to have diverged from the stored paths, and the partial classification algorithm will not be used and the classification procedure will be effectively same as the original one. Otherwise, the classification for an instance (variable ``instance``) starts by following the stored path (``path_diverged = recalc_all = False``) from the root node (``cur_node = dt.root``). The path is followed one node at a time (``cur_node = get_stored_next_node(instance, cur_node)``), in order to look out for mutated nodes along its length, by using the functions ``dt.is_topo_mutated(cur_node)`` and ``dt.is_coeff_mutated(cur_node)``, which signal, respectively, if the current node was mutated via topological mutation or only its test coefficients were mutated. If it was a topological mutation, the instance is facing completely different node, hence the dot product is calculated a new. On the other hand if the current node's test coefficients were mutated, the dot product is reconstructed from the stored value (retrieved via ``get_stored_psum(instance, cur_node)``), using the equation :eq:`dot-product-recalc`. In both cases, it is considered that the instance has diverged from the memorized path: ``path_diverged = True``. The rest of the node test is carried out by comparing the dot product with the threshold to obtain the next node in the path, and if that node corresponds to the next node in the stored path, instance can safely go back to following it (once again ``path_diverged = False``). Finally, in order not to update the memorized paths in each classification run, the argument ``store_paths`` is used to signal to the function whether the mutated DT individual has become the new candidate solution and the updates to the memory should take place.

In the :num:`Table #tbl-delta-time-comp`, the results of the experiment are shown which tests the performance benefits of utilizing the partial reclassification procedure. The DT induction times are listed for...

.. raw:: latex

   \begingroup
   \small
   \renewcommand{\arraystretch}{1}

.. tabularcolumns:: L{0.09\linewidth} | R{0.18\linewidth} R{0.175\linewidth} | L{0.09\linewidth} | R{0.18\linewidth} R{0.175\linewidth}

.. _tbl-delta-time-comp:
.. csv-table:: List of datasets (and their characteristics) from the UCI database, that are used in the experiments throughout this thesis
    :header-rows: 1
    :file: scripts/delta-comp-time.csv

.. raw:: latex

    \endgroup

Complexity of the |algo| algorithm
----------------------------------

The computational complexity of the |algo| algorithm can be calculated using the algorithm pseudo-code. The computational complexity will be given in the big O notation, i.e. the worst-case complexity will be calculated. Since the individual selection is performed in constant time it can be omitted, and the total complexity can be computed as:

.. math:: T(EFTI) = max\_iter\cdot(O(mutate) + O(fitness\_eval))
    :label: cplx_algo_tot_components

The number of leaves, |Nl|, in binary DT is always by 1 larger then the number of non-leaf nodes. If *n* represents the number of non-leaf nodes in the DT, then:

.. math:: N_l = n + 1
    :label: leaves_cnt

In the worst case, the depth of the DT equals the number of non-leaf nodes, hence:

.. math:: D = N_l - 1
	:label: depth

Let |NA| equal the size of the attribute (|x|) and the coefficient (|w|) vectors. Each non-leaf node in the DT has |NA| + 1 (:math:`\theta`) coefficients, and the portion |alpha| is mutated each iteration, so the complexity of mutating coefficients is:

.. math:: T(coefficient\ mutation) = O(\alpha \cdot (N_l - 1) \cdot \NA)
	:label: cplx_mut_coef

The topology can be mutated by either adding or removing the node from the DT. When the node is removed, only a pointer to the removed child is altered so the complexity is:

.. math:: T(node\ removal) = O(1)
	:label: cplx_rem_node

When the node is added, the new set of node test coefficients needs to be calculated, hence the complexity is:

.. math:: T(node\ addition) = O(\NA)
	:label: cplx_add_node

Since :math:`\rho\ll\alpha\cdot (N_l - 1) < \alpha\cdot N_l`, the complexity of the whole DT Mutation task sums to:

.. math:: T(mutation) = O(\alpha \cdot (N_l - 1) \cdot \NA + \rho (O(1)+O(\NA))) = O(\alpha \cdot N_l \cdot \NA)
    :label: cplx_mutation

Once the number of hits is determined, the fitness can be calculated in constant time :math:`O(1)`, hence the complexity of the whole ``fitness_eval()`` function is:

.. math:: T(fitness\_eval) = N_I\cdot O(find\_dt\_leaf\_for\_inst) + O(N_l\cdot N_c) + O(1)
    :label: fitness_eval

where |NI| is the number of instances in the training set and |Nc| is the total number of classes in the classification problem. As for the ``find_dt_leaf_for_inst()`` function, the complexity can be calculated as:

.. math:: T(find\_dt\_leaf\_for\_inst) = D\cdot O(calculate\_node\_test\_sum),
    :label: find_dt_leaf

and the complexity of the node test evaluation is:

.. math:: T(calculate\_node\_test\_sum) = O(\NA)
    :label: node_test_eval

By inserting the equation :eq:`node_test_eval` into the equation :eq:`find_dt_leaf`, and then both of them into the equation :eq:`fitness_eval`, we obtain the complexity for the ``fitness_eval()`` function:

.. math:: T(fitness\_eval) = O(N_{I}\cdot D\cdot\NA + \Nl\cdot N_c)
    :label: fitness_eval_tot

By inserting the equations :eq:`fitness_eval_tot`, :eq:`cplx_mutation`, :eq:`leaves_cnt` and :eq:`depth` into the equation :eq:`cplx_algo_tot_components`, we obtain the total complexity of the |algo| algorithm:

.. math:: T(EFTI) = max\_iter\cdot(N_I\cdot N_l \cdot\NA + N_l\cdot N_c + \alpha \cdot N_l \cdot \NA)
    :label: cplx_all_together

Since :math:`\alpha\cdot N_l \ll N_I\cdot N_l` the mutation insignificantly influences the complexity and can be disregarded. We finally obtain that complexity of the |algo| algorithm is dominated by the fitness evaluation task complexity, and sums up to:

.. math:: T(EFTI) = O(max\_iter\cdot(N_I\cdot N_l\cdot\NA + N_l\cdot N_c))
    :label: cplx_final

It is clear from the equation :eq:`cplx_final` that the ``fitness_eval()`` function is a good candidate for the hardware acceleration, while the mutation tasks can be left in the software since they insignificantly influence the complexity of the |algo| algorithm.

Software implementations
------------------------

PC implementation
.................

- deljenje hits/inst_cnt nije potrebno, jer je uvek isti inst_cnt
- deljenje sa Nc nije potrebno

ARM implementation
..................

DSP implementation
..................

Experiments
-----------

.. raw:: latex

   \begingroup
   \small
   \renewcommand{\arraystretch}{0.8}

.. tabularcolumns:: L{0.15\linewidth} | R{0.07\linewidth} R{0.07\linewidth} R{0.07\linewidth} R{0.07\linewidth} R{0.07\linewidth} R{0.07\linewidth} R{0.07\linewidth} R{0.07\linewidth} R{0.07\linewidth} R{0.07\linewidth}

.. _tbl-max-iter-comp-acc:
.. csv-table:: List of datasets (and their characteristics) from the UCI database, that are used in the experiments throughout this thesis
    :header-rows: 1
    :file: scripts/max-iter-comp-acc.csv

.. tabularcolumns:: L{0.15\linewidth} | R{0.07\linewidth} R{0.07\linewidth} R{0.07\linewidth} R{0.07\linewidth} R{0.07\linewidth} R{0.07\linewidth} R{0.07\linewidth} R{0.07\linewidth} R{0.07\linewidth} R{0.07\linewidth}

.. _tbl-max-iter-comp-size:
.. csv-table:: List of datasets (and their characteristics) from the UCI database, that are used in the experiments throughout this thesis
    :header-rows: 1
    :file: scripts/max-iter-comp-size.csv

.. raw:: latex

    \endgroup

.. subfigstart::

.. _fig-max-iter-comp-size0:

.. figure:: images/max-iter-comp/size0.pdf
    :align: center

    DT size: ger, mushroom, page, psd, sb

.. _fig-max-iter-comp-acc0:

.. figure:: images/max-iter-comp/acc0.pdf
    :align: center

    DT acc: ger, mushroom, page, psd, sb

.. _fig-max-iter-comp-size1:

.. figure:: images/max-iter-comp/size1.pdf
    :align: center

    DT size: jvow, sick, spf, thy, veh

.. _fig-max-iter-comp-acc1:

.. figure:: images/max-iter-comp/acc1.pdf
    :align: center

    DT acc: jvow, sick, spf, thy, veh

.. _fig-max-iter-comp-size2:

.. figure:: images/max-iter-comp/size2.pdf
    :align: center

    DT size: bch, vene, vote, vow, wilt

.. _fig-max-iter-comp-acc2:

.. figure:: images/max-iter-comp/acc2.pdf
    :align: center

    DT acc: bch, vene, vote, vow, wilt

.. _fig-max-iter-comp-size3:

.. figure:: images/max-iter-comp/size3.pdf
    :align: center

    DT size: adult, car, gls, magic, nurse

.. _fig-max-iter-comp-acc3:

.. figure:: images/max-iter-comp/acc3.pdf
    :align: center

    DT acc: adult, car, gls, magic, nurse

.. _fig-max-iter-comp-size4:

.. figure:: images/max-iter-comp/size4.pdf
    :align: center

    DT size: eb, letter, pid, son, w21

.. _fig-max-iter-comp-acc4:

.. figure:: images/max-iter-comp/acc4.pdf
    :align: center

    DT acc: eb, letter, pid, son, w21

.. subfigend::
    :width: 0.48
    :label: fig-max-iter-comp1

    max-iter-comp

.. subfigstart::

.. _fig-max-iter-comp-size5:

.. figure:: images/max-iter-comp/size5.pdf
    :align: center

    DT size: ausc, bank, bc, ca, hep

.. _fig-max-iter-comp-acc5:

.. figure:: images/max-iter-comp/acc5.pdf
    :align: center

    DT acc: ausc, bank, bc, ca, hep

.. _fig-max-iter-comp-size6:

.. figure:: images/max-iter-comp/size6.pdf
    :align: center

    DT size: ctg, cvf, hrtc, hrts, ion

.. _fig-max-iter-comp-acc6:

.. figure:: images/max-iter-comp/acc6.pdf
    :align: center

    DT acc: ctg, cvf, hrtc, hrts, ion

.. _fig-max-iter-comp-size7:

.. figure:: images/max-iter-comp/size7.pdf
    :align: center

    DT size: bcw, irs, liv, lym, pen

.. _fig-max-iter-comp-acc7:

.. figure:: images/max-iter-comp/acc7.pdf
    :align: center

    DT acc: bcw, irs, liv, lym, pen

.. _fig-max-iter-comp-size8:

.. figure:: images/max-iter-comp/size8.pdf
    :align: center

    DT size: krkopt, seg, shuttle, spect, ttt

.. _fig-max-iter-comp-acc8:

.. figure:: images/max-iter-comp/acc8.pdf
    :align: center

    DT acc: krkopt, seg, shuttle, spect, ttt

.. _fig-max-iter-comp-size9:

.. figure:: images/max-iter-comp/size9.pdf
    :align: center

    DT size: cmc, eye, w40, wfr, wine

.. _fig-max-iter-comp-acc9:

.. figure:: images/max-iter-comp/acc9.pdf
    :align: center

    DT acc: cmc, eye, w40, wfr, wine

.. subfigend::
    :width: 0.48
    :label: fig-max-iter-comp2

    max-iter-comp

Conducted experiments were devised to compare the quality of the DTs evolved by the proposed |algo| algorithm, with the DTs inferred by some of the previously proposed algorithms. In particular, DTs were compared by their size and accuracy. All datasets listed in the :numref:`tbl-uci` were used for the induction in the experiments. All reported results are the averages of the five five-fold cross-validation experiments. Experimental setup for each algorithm and each dataset was as follows:

- The dataset D, was divided into 5 non-overlapping sets: :math:`D_1`, :math:`D_2`, ... :math:`D_5`, by randomly selecting the instances from D using uniform distribution
- For the :math:`i^{th}` cross-validation run, where :math:`i \in (1,5)`, training set was formed by using all the instances from D except the ones from :math:`D_i`, :math:`train\_set = D \setminus D_i`, and was used to induce the DT by the current algorithm being tested
- Inferred DT was than tested for accuracy by using the instances form the set :math:`D_i`.

This whole procedure was repeated 5 times, resulting in 25 inferred DTs for each dataset and for each inference algorithm, together with the classification accuracy calculated as a percentage of correctly classified test set instances, for each of them. Using test set classification accuracies, calculated as a percentage of correctly classified test set instances, average DT classification accuracy for every dataset and DT inference algorithm has been also calculated. Both the DT size and test set classification accuracy are reported with 95% confidence intervals.

For DT inference algorithms that require DT pruning a pruning set has been created, taking 30% of the training set instances selected randomly, and used to prune the DT.

In the performance comparison process, following incremental DT inference algorithms have been used:

# OC1-AP, Oblique Classifier Algorithm developed and used by S. K. Murthy and others in [16], but limited to using only axis-parallel tests,
# OC1, developed and used by S. K. Murthy and others with default parameters [16],
# CART-LC, as used by Murthy in [16],
# OC1-ES, extension to OC1 using evolution strategies described in [17],
# OC1-GA, extension to OC1 using genetic algorithms described in [17],
# OC1-SA, extension to OC1 using simulated annealing described in [17] and
# HBDT, proposed in [21], that uses HereBoy algorithm for hyperplane optimization process.

DTs generated by algorithms 1-6 have used error complexity pruning algorithm [37] for DT pruning, while DTs generated by the algorithm 7 have been pruned by the Prune_DT algorithm proposed in [21].

In addition, two full DT induction algorithms, previously proposed in the open literature have also been used:
# GaTree algorithm proposed by the Papagelis and Kalles in [22] and
# GALE evolutionary model proposed by Llorà and Wilson in [24].

Since these algorithms infer optimized DTs, no additional pruning algorithm and pruning set are needed.

Average test set accuracies for all DT inference algorithms that have been used in the experiments, and average tree sizes are presented in Tables 1 and 2 respectively. For every reported value, 95% confidence interval is also provided.

.. raw:: latex

   \begingroup
   \scriptsize
   \setlength{\tabcolsep}{.1em}

.. tabularcolumns:: p{0.05\linewidth} *{10}{R{0.095\linewidth}}

.. csv-table:: Average Test Set Accuracies on Selected Data Sets from the UCI Database for Different DT Inference Algorithms
    :header-rows: 1
    :file: data/efti_experiments/accuracy.csv

.. tabularcolumns:: p{0.05\linewidth} *{10}{R{0.095\linewidth}}

.. csv-table:: Average Tree Sizes on Selected Data Sets from the UCI Database for Different DT Inference Algorithms
    :header-rows: 1
    :file: data/efti_experiments/size.csv

.. raw:: latex

    \endgroup

In order to discover was there a statistical difference among the estimated test set accuracies and tree sizes of the ten DT inference algorithms one-way analysis of variance (ANOVA) [38] has been applied on collected data with the significance level set at 0.05. If ANOVA analysis indicated that at least one of the results was statistically different for the others, Tukey multiple comparisons test [39] was used to group the algorithms into groups of statistically identical results. Bold values in Tables 1 and 2 indicate algorithms with the best results for every dataset that was used in the experiments.

From Table 1 it can be seen that proposed EFTI algorithm has the best accuracy, or is in the group of algorithms with the best accuracy for 16 out of 21 datasets that were used in the experiments. When induced tree size is considered, performance of the EFTI algorithm is even better. EFTI algorithm induces smallest DTs for 18 out of 21 selected datasets.
In order to better estimate improvements in the accuracy and tree size of DTs inferred using proposed EFTI algorithm, Tables 3 and 4 present the relative increase and decrease in DT accuracy and size when DTs induced using nine other, previously proposed algorithms,  are compared with the results obtained using EFTI algorithm.

.. raw:: latex

   \begingroup
   \footnotesize
   \setlength{\tabcolsep}{.1em}

.. tabularcolumns:: p{0.05\linewidth} *{9}{R{0.1\linewidth}}

.. csv-table:: Percentage DT size increase (if reported value is positive) or decrease (if reported value is negative)
    :header-rows: 1
    :file: data/efti_experiments/accuracy_delta.csv

.. tabularcolumns:: p{0.05\linewidth} *{9}{R{0.1\linewidth}}

.. csv-table:: Percentage of accuracy increase (if reported value is positive) or decrease (if reported value is negative)
    :header-rows: 1
    :file: data/efti_experiments/size_delta.csv

.. raw:: latex

    \endgroup

Table 3 presents relative increase or decrease in the tree size when size of the DTs induced by the EFTI algorithm is compared with size of DTs induced by other algorithms that have been used in the experiments. Values in the Table 3 are expressed in terms of percentages. If reported value is negative, this indicates that the DT induced by EFTI algorithm is smaller than the DT induced by some of the previously proposed algorithms. Positive value indicates that the DT induced by EFTI algorithm is bigger than the DT induced by some of the previously proposed algorithm. Using this notation, large negative values are preferred, because they indicate that EFTI is able to induce significantly smaller DTs that previously proposed algorithms.

Table 4 presents results of percentage increase or decrease in DT accuracy when EFTI built DTs are compared with DTs induced by previously proposed algorithms. In this case large positive values would be preferred, since they would indicate that the DTs built by EFTI are significantly more accurate than DTs built using previously proposed algorithms. In both tables, last row presents the average results in size and accuracy increase/decrease of EFTI DTs over DTs induced by previously proposed algorithms.

From Table 3 it can be seen that EFTI algorithm is able to induce significantly smaller DTs when compared with previously proposed DT building algorithms. When compared with incremental DT building algorithms (OC1-AP, OC1, CART-LC, OC1-SA, OC1-GA, OC1-ES and HBDT) relative decrease in DT size ranges from 21.07%, in case of OC1-GA, up to 65.69% in case of OC1-ES. This is a very significant decrease, especially when hardware implementation is concerned. EFTI built DTs would require from 21.07% to 65.69% less hardware resources when compared with DTs that were induced using some of the previously proposed algorithms. When compared with full DT induction algorithms (GALE and GaTree), EFTI shows less improvement, but is still able to induce DTs that are on average -8.40% and -12.14% smaller.

When DT accuracy is concerned, from Table 4 it can be seen that when compared with incremental DT building algorithms, EFTI is inducing DTs with almost identical accuracy. For most incremental algorithms increase in DT accuracy, when using EFTI, is only a couple of percentages. The only significant improvement is made in case of OC1-GA, 11.66%. On the other hand, when accuracy of DTs induced by EFTI is compared with accuracies of DTs built with previously proposed full DT inference algorithms (GALE and GaTree), increase in DT accuracy is more significant, 15.36% and 12.67% respectively.
