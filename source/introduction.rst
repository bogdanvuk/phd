.. include:: conf.rst

.. _ch-introduction:

Introduction
============

The research on decision trees is a part of a broather field called machine learning, which in turn is a branch of the artificial intelligence. The machine learning techniques are usefull for solving problems when:

- There exists a lot of input data on the problem, but no algorithm (or no efficient one) to produce the output based on the input data
- Either the problem changes with time, or some of its characteristics are not known at the design time, hence an adaptable solution is needed, when the new circumstances arise

Because of the ever increasing penetration of the machine learning systems into the embedded world, and its even greater potential for in the future, the presented induction algorithms have been tailored for implementation by the embedded systems, in that they use less resources for the operation than the existing solutions. One way of reducing the resource consumption is to induce and thus operate on smaller decision trees. Furthermore, the smaller decision trees also represent a more succint solution to the problem, which is always prefered in science (Occam's razor). Hence, the main motivation for this dissertation was to develop the decision tree induction algorithms that:

1. induce smaller DTs than the existing solutions without the loss of accuracy,
2. can be effitiently used in embedded applications, and
3. are easily parallelizable and hence accelerated in hardware

For big datasets, which are common in practice, the presented decision tree induction algorithms are very time consuming. Hence, the hardware accelerators for |algo| and |ealgo| algorithms are also proposed, namely |cop| and |ecop| co-processors, that significantly reduce their times of execution. Furthermore, the implementations of the proposed induction algorithms that utilize these hardware accelerators are also described.

Machine learning
----------------

Our ever-improving capabilities in collecting the data from the world and constant increase in processing power available to us, have significantly changed our approaches to problem solving in recent decades. Science has also taken advantage of the computers' ability to store massive amounts of data. Ever since it became possible to sequence proteins and the DNA molecule some time after that, immense datasets started emerging from the scientific research in the field of biology, which was followed shortly by other sciences as well. Ever-increasing number and power of telescopes used in astronomy produce larger and larger pools of raw data, with Hubble for an example generating about 140 Gb of raw data each week. Equally, medical science large datasets arise from storing the outcomes of medical tests. The Human Connectome Project aims at mapping the human connectome of a large number of adults and has generated around 2 terabytes of data at the time of writing, and CERN data center processess about 1 petabyte of data each day.

The size and complexity of these datasets mean that humans are unable to extract useful information therefrom, without the help of sophisticated and efficient algorithms. The scientific field that studies the systems that make use of the abundance of the available data and computational power to solve problems is called the machine learning. The machine learning :cite:`flach2012machine,murphy2012machine` is a branch of artificial intelligence that studies algorithms and systems that improve their performance with experience, i.e. that can "learn" from the data. In other words, machine learning is about making computers modify or adapt their actions (whether these actions are making predictions, or controlling a robot) so that these actions get more accurate, where accuracy is measured by how well the chosen actions reflect the desired ones. Of particular interest are, of course, the problems that haven't been satisfactorily solved using other methods.

For an example, one of the challenges to whose solution the machine learning contributed greatly is the problem of self-driving vehicles. There are many aspects of automated driving which are best solved by some type of machine learning system. First of all, the vehicle must be made aware of its surroundings in three dimensions, usually by some kind of camera that continously provides the vehicle with images of the space around it. The final goal of this task is to recognize the objects of interest for driving: road lines to follow, pedestrians and other obstacles to avoid, road signs to acknowledge, etc. The object recognition is usually performed in two steps :cite:`levinson2011towards,khan2011image`:

- **clustering** of the image pixels that probably belong to the same object into so called regions of interest (ROI) (also called image segmentation), and
- **classification** of ROIs into classes of known objects

Second of all, based on the surroundings and the driving directions given by the vehicle user, the vehicle needs to devise and maintain a driving strategy, i.e. to determine control signals to vehicle actuators (the steering wheel, gas and break pedals, etc.) in order to, among others, maximize the driving speed within the current speed limit, minimize the risk of colision, etc. These three tasks: the pixel clustering, the ROI classification and driving strategy development are usually solved using machine learning systems that are all induced (built) using different learning strategies, which will be discussed below.

The :num:`Figure #fig-machine-learning-overview` shows an overview of how machine learning is used to address a given task as described in :cite:`flach2012machine`. A task has a goal of solving a certain problem of interest regarding the objects of the problem domain, which are in turn defined in terms of its attributes (also called features). The choice of attributes defines a ‘language’ in which all the objects in the problem domain get their relevant aspects decribed. Once a suitable attribute representation is selected, the machine learning system will not be concerned with the domain objects themselves, but only operatate on their attribute representations. Domain objects are usually represented in the form of an attribute vector, also called an instance (since it acts as a problem instance for the machine learning system), which lists the values of all object attributes. Hence, the goal is to obtain an appropriate mapping for a task, called a model, from attributes to the desired outputs, which in turn correspond to the outputs of the problem that is being solved by the machine learning system. Obtaining such a model from training data is what constitutes a learning problem.

.. _fig-machine-learning-overview:

.. bdp:: images/machine_learning_overview.py

    An overview of how machine learning is used to solve problems in certain domain. The domain objects are represented by their attribute vectors, which are in turn called the instances. A machine learning task is defined to describe the desired output of the system in terms of the instances. The model that produces the correct output for the input instances, i.e. solves the defined task, is obtained via process of learning on the tranining set.

Machine learning systems can be constructed using supervised learning, unsupervised learning or any combination of the two techniques :cite:`flach2012machine,murphy2012machine`. Supervised learning implies providing the desired responses to the instances of the training set to construct the system, while unsupervised learning implies constructing the system based on the instances only. When the supervised learning is used, the lifetime of a machine learning system usually comprises two distinct phases:

- the training (induction or learning), during which the learning problem is solved and the model is developped, and
- the deployment, during which the model is used to process new data

For an example, the classification of ROIs for self-driving vehicles is usually performed by the machine learning systems, induced by the method of supervised learning. During the training phase, a training set is used to build the system, which comprises input data instances and the desired system responses to them. Once constructed, the system is ready to be used, where new, previously unseen data, will arrive and the system must provide the responses using the knowledge extracted from the training set.

When using unsupervised learning, the correct responses to the input data are not provided, instead the algorithm tries to identify similarities between the inputs, so that instances that have something in common solicit similar outputs. The statistical approach to unsupervised learning is known as density estimation. The clustering of image pixels to obtain ROIs for self-driving vehicles is an example of machine learning system that uses unsupervised learning. The system is never trained with the examples on how to map pixel groups to ROIs (since there are too many possible correct mappings), but has to apprehend it on its own, based on the attributes the pixels in a group share.

Reinforcement learning is somewhere between supervised and unsupervised learning. The learning algorithm gets told when the answer is wrong, but without the advice on how to correct it. It has to explore and try out different possibilities until it discovers how to get the answer right. Reinforcement learning is sometime called learning with a critic, because of the monitor that scores the answer, but does not suggest improvements. Developping the right driving strategies for self-driving vehicles is usually performed by the machine learning system that was trained using the reinforcement learning procedure. To provide for learning purposes the right combination of the positions of the steering wheel, acceleration and breaking pedals, etc. in each time instant, with dynamic circumstances, would be an impossible task to perform. Hence, in order to develop the correct driving strategies, the machine learning system can be let to drive the vehicle and be given positive or negative feedback during the process based on some general parameters, for an example: the driving speed or the distance it holds from the objects around.

One of the main features of machine learning systems is the power of generalization, allowing them to perform well on new, unseen data instances, after having experienced a learning procedure. It is of special interest to maintain the power of generalization of the system being trained by the supervised learning method. A machine learning problem may have multiple solutions, i.e. multiple models can perform equally well on the training set. If care is not taken, it is possible for the induced machine learning system to perform excelently on the training set, but fail when used on new data. This phenomenon is called overfitting, in that the induced model learned too many features of the training set that are not shared by other problem instances, i.e. the model was made to overly fit the training set. Good performance on the training data is only a means to an end, not a goal in itself, since it is the performance on the new data that should be maximized. By maximizing the induced model's power of the generalization also has the result that the it can better deal with noise, which represents small inaccuracies in the data that are inherent in measuring any real world process. The model must not take the instance attribute values too literaly, but should expect that each of them has some noise superimposed.

The machine learning systems can perform various tasks, such as classification, regression, clustering, etc. The classification implies categorizing problem instances in some number of discrete classes. Sometimes it is more natural to abandon the notion of discrete classes altogether and instead predict a real number, i.e. perform the task which is called regression. The task of grouping data without prior information on the groups is called clustering, which usually uses models induced by the method of unsupervised learning. A typical clustering algorithm works by assessing the similarity between instances (the things we’re trying to cluster, e.g., connected pixels) and putting similar instances in the same cluster and ‘dissimilar’ instances in different clusters. There are many other patterns that can be learned from data in an unsupervised way. Association rules are a kind of pattern that are popular in marketing applications, and the result of such patterns can often be found on online shopping web sites.

In the open literature, a range of machine learning systems have been introduced, including decision trees (DTs) :cite:`rokach2007data,rokach2005top`, support vector machines (SVMs) :cite:`abe2005support` and artificial neural networks (ANNs) :cite:`haykin2009neural`. Data mining is a field where machine learning systems have been extensively used :cite:`witten2005data`, among which DTs, ANNs and SVMs are the most popular :cite:`rokach2007data,wu2009top,wang2006data`.

Decision Trees
--------------

Widely used machine learning model for classification tasks is a DT classifier. The classification process by the DT can be depicted in a flowchart-like tree structure given in the :num:`Figure #fig-dt-traversal`. Due to their comprehensible nature, which resembles the process of human reasoning, DTs have been widely used to represent classification models. Amongst other machine learning algorithms DTs have several advantages, such as the robustness to noise, the ability to deal with redundant or missing attributes, the ability to handle both numerical and categorical data and the facility of understanding the computation process. Furthermore the computational cost of using the DT is quite low: :math:`O(log{N})`, where *N* is the number of DT nodes.

.. _fig-dt-traversal:

.. bdp:: images/dt_traversal.py

    The classification process by the binary DT.

In theory, DTs can have an arbitrary branching factor (n-ary DTs), but the binary DTs (with the branching factor of 2), i.e. the DTs with only two children per node, are used most often for being easier to implement. Furthermore, a tree with an arbitrary branching factor can always be represented by a functionally equivalent binary DT. The :num:`Figure #fig-dt-traversal` shows the process of classification by a binary DT. The DT in the figure consists of 4 nodes represented by circles numbered 1, 2, 3 and 6. The DT also has 5 leaves represented by squares numbered 4, 5, 7, 8 and 9, where each of the leaves has a class assigned to it (:math:`C_{1}` through :math:`C_{5}` in this example). The classification is performed by letting instances traverse the tree, starting from the root (enumerated as 1), until they reaches any of the leaves. Depending on the leaf in which the instance finishes the traversal, it is classified into the class assigned to that leaf.

Each of the DT nodes is assigned a test: :math:`T_{1}`, :math:`T_{2}`, :math:`T_{3}` and :math:`T_{6}` in this example. In each node the instance visits during its traversal through the DT, the node test is used to determine through which of the node's children will the traversal continue, based on the instance's attribute values. In case of a binary DT, the node test decision is likewise binary. If the test evaluates to **true** (T), the DT traversal is continued via the left child, otherwise if it evaluates to **false** (F), it is continued via the right child. The final path of the instance through the DT depends on the test results in all the nodes the instance encounters during the traversal.

Each machine learning problem needs to have defined domain, which is in turn defined as the set of all domain objects. First, the set of attributes is chosen to uniquely represent the domain objects in form of the attribute vector - |x|. Also, the domain of each attribute needs to be defined, where there are usually two choices:

- the domain can be a finite set of unordered values, in which case the attribute is called categorical, or
- the domain can be a subset of the set of the real numbers, in which case the attribute is called numerical.

The set of all possible attribute vectors forms the |NA| - dimensional attribute space, where |NA| is the number of attributes that are used to describe the domain object, i.e. the size of the attribute vector |x|. In the context of the attribute space, each binary DT node test splits the space into two regions, one containing all the instances for which the test produced the result **true** and the other containing the rest of the instances, for which the test evaluated to **false**. Each DT node can be thus assigned a sub-region of the attribute space, that in turn contains all the instances that pass through that node during their traversal of the DT. Hence, each node splits the region assigned to it by into two sub-regions and assigns each of them to one of its children. This process of attribute space partitioning starts from the DT root, which is assigned whole attribute space (every instance needs to visit the root node), and continues downwards to the DT leaves. The final result of this process is a clear partition of the attribute space into a number of disjoint regions, each associated with one leaf node. Each of this regions in the partition can thus be assigned the associated leaf's class, meaning that all the instances contained in the region will be classified into that class.

Based on the characteristics of the functions implementing the node tests, the DTs can be categorized into: orthogonal, oblique and nonlinear (**what about univariate and multivariate terminology**). The names of the categories were derived from the shape of the hypersurface defined by their tests. Hence, the orthogonal DTs divide the attribute space using the orthogonal hyperplanes, the oblique DTs using oblique hyperplanes, and nonlinear DTs using nonlinear hyperplanes.

This thesis focuses on the oblique binary classification DTs. The tests performed by an oblique DT in each node are afine and have the following form:

.. math:: \mathbf{w}\cdot \mathbf{x} = \sum_{i=1}^{N_A}w_{i}\cdot x_{i} < \theta,
    :label: oblique-test

where |w| represents the coefficient vector and |th| (called the threshold) models the afine part of the test.

Next, an example describing the classification process by oblique DTs will be given. The :num:`Figure #fig-oblique-dt-traversal-attrspace-only` shows a dataset named yinyang that will be used for this example, plotted in its attribute space. The dataset instances are conviniently described using only two attributes :math:`x_1` and :math:`x_2`, so that they can be represented in 2-D attribute space. The dataset comprises instances belonging to one of the two classes: :math:`C_1` and :math:`C_2`. Each instance is represented in the figure by either a star (if it belongs to the class :math:`C_1`) or by a square (if it belongs to the class :math:`C_2`), with its position defined by the values of its attributes.

.. _fig-oblique-dt-traversal-attrspace-only:
.. plot:: images/oblique_dt_traversal_attrspace_only.py
    :width: 90%

    The yinyang dataset used for the demonstration of the classification process by oblique DTs. Instances of the dataset are described using two attributes :math:`x_1` and :math:`x_2`, and can belong to one of the two classes :math:`C_1`, represented by the star symbols, and :math:`C_2`, represented by the square symbols.

An example of the oblique binary DT that can be used to accurately classify the instances of the yinyang dataset, is shown in the :num:`Figure #fig-oblique-dt-traversal`. Since this is an oblique DT, each of its node tests follow a form defined by the equation :eq:`oblique-test`. Each DT leaf has one of two classes of the yinyang dataset assigned to it. The classification is performed by letting each instance of the yinyang dataset traverse the DT, starting from the root node, in order to be assign a class to it. During the traversal, the node's test is evaluated at each of the DT nodes. Based on the results of the node test conditions (**true** or **false**), the DT traversal is continued accordingly until a leaf is reached, when the instance is classified into the class assigned to that leaf. One possible traversal path is shown in the :num:`Figure #fig-oblique-dt-traversal`, where the instance got classified into the class :math:`C_{1}` after the traversal.

.. _fig-oblique-dt-traversal:

.. bdp:: images/oblique_dt_traversal.py

    Oblique binary DT that could be used to classify the instances of the yinyang dataset ploted in the :num:`Figure #fig-oblique-dt-traversal-attrspace-only`. The curvy line shows a traversal path for one possible instance. This example traversal path is visually presented via plots in dataset attribute space in the :num:`Figure #fig-oblique-dt-traversal-attrspace`.

As it was already discussed, a different way of looking at the classification process by the DT is by examining what happens in the attribute space. The structure of the attribute space regions is defined by the DT node tests, resulting in one region assigned to each node and each leaf of the DT as shown in the :num:`Figure #fig-oblique-dt-attrspace`. The dashed lines on the figure represent the 1-D hyperplanes (lines in this case) generated by the node tests that partition the attribute space. The regions of the final partition are the ones assigned to the DT leaves, and each of them is marked with the ID of its corresponding leaf and the class assigned to that leaf. The regions assigned to the non-leaf nodes can be easily obtained from the figure plot and the DT structure from the :num:`Figure #fig-oblique-dt-traversal`, by noticing that the node's region equals the union of its children regions. Working from the bottom up recursively, regions for all DT nodes can be obtained by combining the regions assigned to their descendents.

.. _fig-oblique-dt-attrspace:

.. plot:: images/oblique_dt_traversal_attrspace_0.py
    :width: 80%

    The attribute space partition of yinyang dataset from the :num:`Figure #fig-oblique-dt-traversal-attrspace-only` generated by the DT from the :num:`Figure #fig-oblique-dt-traversal`. The instances belong to one of the two different classes: :math:`C_1` marked by stars and :math:`C_2` marked by squares. The dashed lines on the figure represent the hyperplanes generated by the node's tests that partition the attribute space into the regions, each corresponding to a leaf of the DT. Each of the attribute space regions is marked with the ID of its corresponding leaf and the class assigned to the leaf.

In order to find out in which region the instance resides, and thus to which class it belongs, we need to let the instance traverse the DT. The :num:`Figure #fig-oblique-dt-traversal-attrspace` shows this process for the example traversal path shown in the :num:`Figure #fig-oblique-dt-traversal`. At the begining, when the classification of an instance is started at the root, all the regions are valid candidates. After the root node test is evaluated, the location of the instance can be narrowed down to the regions either to the left or to the right of the hyperplane :math:`\mathbf{w_1}\cdot \mathbf{x} - \theta = 0`, generated by the root node test. For this example instance, the root node test evaluated to **true**, the instance continues to the node 2, and the location of the instance is narrowed down to the region assigned to the node 2 and shown in the :num:`Figure #fig-oblique-dt-traversal-attrspace-1`. Then, the test of the node 2 is evaluated for the instance, and it turns out to be **false**, hence the instance continues to the node 5 and the number of possible regions is reduced again to the ones marked in the :num:`Figure #fig-oblique-dt-traversal-attrspace-2`, i.e. to the part of the attribute space assigned to the node 5. Finally, the node 5 test is evaluated to **true**, the instance hits the leaf node 8 and it is finaly located in the region marked in the :num:`Figure #fig-oblique-dt-traversal-attrspace-3` and assigned the :math:`C_1` class.

.. subfigstart::

.. _fig-oblique-dt-traversal-attrspace-1:

.. plot:: images/oblique_dt_traversal_attrspace_1.py
    :align: center

    Region of the attribute space assigned to the node 2 of the DT from the :num:`Figure #fig-oblique-dt-traversal`.

.. _fig-oblique-dt-traversal-attrspace-2:

.. plot:: images/oblique_dt_traversal_attrspace_2.py
    :align: center

    Region of the attribute space assigned to the node 5 of the DT from the :num:`Figure #fig-oblique-dt-traversal`.

.. _fig-oblique-dt-traversal-attrspace-3:

.. plot:: images/oblique_dt_traversal_attrspace_3.py
    :align: center

    Region of the attribute space assigned to the node 8 of the DT from the :num:`Figure #fig-oblique-dt-traversal`.

.. subfigend::
    :width: 0.48
    :label: fig-oblique-dt-traversal-attrspace

    The figure shows the attribute space regions assigned to the nodes and leafs an example instance visits during its traversal along the line shown in the :num:`Figure #fig-oblique-dt-traversal`.

Decision tree induction
-----------------------

In the field of machine learning, as is with most other scientific disciplines, simpler models are prefered over the more complex ones as stated in the principle of Occam's razor :cite:`gauch2003scientific`. The same principle, but in terms of the information theory, was proposed in :cite:`rissanen1985minimum` under the name Minimum Description Length (MDL). In essence, it says that the shortest description of something, i.e. the most compressed one, is the best description. The preference for simplicity in the scientific method is based on the falsifiability criterion. For each accepted model of a phenomenon, there is an extremely large number of possible alternatives with an increasing level of  complexity, because aspects in which the model fails to correctly describe the phenomenon can always be masked with ad hoc hypotheses to prevent the model from being falsified. Therefore, simpler theories are preferable to more complex ones because they are more testable. Hence, there is an obvious benefit for having the algorithm that induces smaller DTs, since smaller DT corresponds to a simpler description of a phenomenon being modeled by it.

Second, with growth and advancements in the field of electronics, wireless communications, networking, cognitive and affective computing and robotics, embedded devices have penetrated deeper into our daily lives. In order for them to seemlesly integrate with our daily routine, they need to comprise some sort of machine learning system for execution of any non-trivial task. Hence, the |algo| algorithm, proposed in this thesis, was designed with its implementation for the embedded systems in mind. In other words, the |algo| algorithm was designed to reqire as little hardware resources for implementation as possible in order for it to be easily integrated into an embedded system.

The DT induction phase can be very computationally demanding and can last for hours or even days for practical problems, especially when run on the less powerfull, embedded processors. By accelerating the |algo| algorithm in hardware, the machine learning systems could be trained faster, allowing for shorter design cycles, or could process larger amounts of data, which is of particular interest if the DTs are used in the data mining applications :cite:`witten2005data`. This might also allow the DT learning systems to be rebuilt in real-time, for the applications that require such rapid adaptation, such as: machine vision :cite:`prince2012computer,challa2011fundamentals,ali2010hardware,tomasi2010fine`, bioinformatics :cite:`lesk2013introduction,baldi2001bioinformatics`, web mining :cite:`liu2007web,russell2013mining`, text mining :cite:`weiss2010fundamentals,aggarwal2012mining`, etc. Hence, the |algo| algorithm was designed to be parallel in nature and thus be easily accelerated by an application specific co-processor. Furthermore, some of the world leading semiconductor chip makers offer the solutions which consist of a CPU integrated with an FPGA, like Xilinx with its Zynq series and Intel with its new generation Xeon chips. The hardware accelerated implementation of the |algo| algorithm can be readily implemented on these devices, with the hardware for the |algo| algorithm acceleration built for the integrated FPGA.

General approaches to DT induction
..................................

Finding the smallest DT consistent with the training set is an NP-hard problem :cite:`murthy1994system`, hence, in general it is solved using some kind of heuristic. The DT is said to be consistent with the training set if and only if it classifies all the training set instances in the same way as defined in the training set. There are two general approaches to DT induction using supervised learning: incremental (node-by-node, also known as Top-Down Induction of Decision Trees, or TDIDT) and nonincremental (or full tree) induction.

The incremental approach uses greedy top-down recursive partitioning strategy of the training set for the tree growth. The algorithm starts with an empty DT and continues by forming the root node test and adding it to the DT. In the attribute space, the root node test splits the training set in two partitions, one that will be used to form the root's left child subtree, and the other the right child subtree. In other words, the root node is assigned the whole training set, which is partitioned in two by the root node test and each partition is assigned to one of the root's two children. The node test coefficients are optimized in the process of maximizing some cost function measuring the quality of the split. Iteratively, the nodes are added to the DT, whose tests further divide the training set partitions assigned to them. If the node is assigned a partition of the training set where all instances belong to the same class, i.e. the partition is clean, no further division is needed and the node becomes the leaf with that class assigned to it. Otherwise, the process of partitioning is continued until only clean partitions remain. In this stage, the induced DT is considered overfitted, i.e it performs flawlessly on the training set, but badly on the instances outside the training set. The common approach for increasing the performance of the overfitted DT on new instances is prunning.

This approach is considered greedy in the sense that the node test coefficients (coefficient vector |w| and threshold value |th|) are optimized by examining only the part of the training set assigned to the current node, i.e. based on the "local" information. The information on how the training set partitions are handled in other subtrees of the DT (subtrees not containg the node currently being inserted into the DT) are not used to help optimize the test coefficients. Furthermore, by the time the node has been added to the DT and the algorithm continued creating other nodes, the situation has changed and the new information is available, but it will not be used to further optimize the test of the node already added to the DT. This means that only some local optimum of the induced DT can be achieved.

Incremental algorithms use a simpler heuristic and are computationally less demanding than the full DT inducers. However, the algorithms that optimize the DT as a whole, using complete information during the optimization process, generally lead to more compact and possibly more accurate DTs when compared with incremental approaches. Furthermore, the DTs can be induced both using only axis-parallel node tests or using oblique node tests. The advantage of using only axis-parallel tests is in reduced complexity as the task of finding the optimal axis-parallel split of the training set is polynomial in terms of |NA| and |NI|. More precisely, the optimization process needs to explore only :math:`N_A \cdot N_I` distinct possible axis-parallel splits :cite:`murthy1994system`. On the other hand, in order to find the optimal oblique split, total of :math:`2^d \cdot \binom{N_A}{N_I}` posible hyperplanes need to be considered, making it an NP-hard problem. On the other hand, the DTs induced with oblique tests offten have much smaller number of nodes than the ones with axis-parallel tests. Hence, in order to fullfill its goal of inducing smaller DTs than existing solutions, the |algo| algorithm needs to implement oblique full DT induction.

Various algorithms for incremental DT induction have been proposed in the open literature. The ID3 algorithm proposed in :cite:`quinlan1986induction` was designed to operate mainly on categorical attributes. In the DT created by the ID3 algorithm, each node test operates on a single attribute only. The number of outcomes the test can produce equals the number of different values the attribute can take, and the attribute space will be split into the same number of regions by the test. In order to choose which attribute should be used for the test in a node, the information gain (IG), given by the equation :eq:`eq-information-gain`, is calculated for all possible attributes. The information gain is a difference between the information entropy of the attribute space region assigned to the node, and the combined entropies of the regions produced by the node test split.

.. math:: IG(A_i,S) = H(S) - \sum_{t \in T}p(t)H(t),
    :label: eq-information-gain

where :math:`H(S)` is information entropy of the region assigned to the node, T is the partition in subregions generated by the node test based on the attribute :math:`A_i`, :math:`p(t)` is the proportion of the number of elements in subregion :math:`t` to the number of elements in the region assigned to the node :math:`S` and :math:`H(t)` is the information entropy of the subregion :math:`t`. The attribute whose test would produce the largest IG is selected for the node. As an improvement to ID3, the C4.5 algorithm was published in :cite:`quinlan1993c4`. C4.5 introduced the possibility to handle continuous attributes, to handle instances whose attributes are missing and introduced the prunning step after the DT has been created.

The Classification and Regression Tree (CART) algorithm was introduced in :cite:`breiman1984classification`, that unlike ID3 induces binary DTs. Similar to ID3 only a single attribute's value is tested in each node test, hence CART produces axis-parallel binary splits. When searching for the best test for a node, CART evaluates every possible way in which attribute domain could be split in two, hence the attribute domains need to be discrete and finite. Various measures could be used for selecting the best split: Gini index, Twoing, information entropy, etc. An extension to CART that generates oblique tests has also been proposed in :cite:`breiman1984classification` by the name CART with linear combinations or CART-LC. The OC1 algorithm was proposed in :cite:`murthy1994system`, which improves upon the CART-LC algorithm. While considering the best split for a DT node, OC1 first searches for the best axis-parallel test for the node. OC1 then tries to produce an oblique test that will outperform it, and if that fails, the algorithm defaults to the axis-parallel test. Furthermore, unlike CART-LC that is fully deterministic, OC1 incorporates the ideas from simulated annealing algorithm, which address the issue of escaping local optima and enable OC1 to produce different DTs from a single training set. Various extensions to OC1 algorithm based on evolutionary algorithms were introduced in :cite:`cantu2003inducing`, namely: OC1-ES (OC1 extension using evolution strategies), OC1-GA (OC1 extension using genetic algorithms) and OC1-SA (OC1 extension using simulated annealing). These extensions were specifically employed in the process of searching for the best oblique split. The authors of C4.45 and C4.55 algorithm claim in :cite:`mahmood2010novel` to have acheived performance superior to C4.5 algorithm with respect to both accuracy and size, by using various optimizational techniques to improve upon original C4.5 algorithm.

The Univariate Margin Tree (UMT) algorithm given in :cite:`yildiz2012univariate`, borrows the ideas from linear SVMs in the way it tries to find the optimal split for a node. Fisher's decision tree algorithm for incremental oblique DT induction, proposed in :cite:`lopez2013fisher`, implements yet a different strategy for obtaining the split using Fisher's linear discriminant, and reported obtaining smaller DTs, with shorter induction time without the loss in accuracy. A bottom-up induction approach was explored in :cite:`barros2014framework`, resulting in the Bottom-Up Oblique Decision-Tree Induction Framework (BUTIF). This algorithm operates by clustering the instances based on their classes and position in the attribute space, and asssigning those clusters to the leaf nodes prior to creating the trunk of the DT. Starting from the formed leaves, the BUTIF algorithm generates the DT by merging the existing subtrees until finally the root is formed. In :cite:`struharik2014inducing`, authors employed the HereBoy evolutionary algorithm to optimize the positions of the node test hyperplanes.

:cite:`islam2010explore,liu2011improved,manwani2012geometric`.

The other approach to the DT inference is the full DT induction. In this approach a complete DT is manipulated during the inference process. Acording to some algorithm, the tree nodes are added or removed, and their associated tests are modified. Considerable number of full DT inference algorithms has been also proposed. A genetic algorithm operating on full DTs as individuals, called GaTree, was introduced in :cite:`papagelis2000ga`. Another algorithm based on genetic algorithms, called GALE and proposed in :cite:`llora2004mixed`, attempted to extract additional parallelism from the induction process by employing ideas from the field of cellular automata. In :cite:`bot2000application`, genetic programming was employed to create a nested structure of IF-THEN-ELSE statements that is homologous to a DT. Finally, the ant colony optimization technique was used for the algorithms introduced in :cite:`otero2012inducing,boryczka2015enhancing`.

:cite:`krketowski2005global`.


Evolutionary oblique full DT induction
......................................

Since the process of finding the optimal oblique DT is a hard algorithmic problem, most of the oblique DT induction algorithms use some kind of heuristic for the optimization process, which is often some sort of evolutionary algorithm (EA). The :num:`Figure #fig-evolutionary-dt-algorithm-tree` shows the taxonomy of EAs for the DT induction as presented in :cite:`barros2012survey`.

.. _fig-evolutionary-dt-algorithm-tree:

.. figure:: images/taxonomy.pdf

    The taxonomy of evolutionary algorithms for DT induction as presented in :cite:`barros2012survey`.

The evolutionary algorithms for inducing DTs by global optimization (the full DT induction) are usualy some kinds of Genetic Algorithms :cite:`papagelis2000ga,llora2004mixed,krketowski2005global`, which in turn operate on a population of candidate solutions. The typical populations used by these algorithms contain tens or even hundereds of individuals. In order to save on needed resources for the implementation, the |algo| algorithm was based on HereBoy :cite:`levi2000hereboy` evolutionary algorithm which operates on a single candidate solution, hence, the |algo| algorithm requires one or even two orders of magnitude less hardware resources for the implementation then the existing evolutionary algorithms. HereBoy is an evolutionary algorithm that combines features from Genetic Algorithms and Simulated Annealing and operates on the bitstring representation of the individual being evolved. The |algo| algorithm (as well as HereBoy algorithm) operates only on a single candidate solution and single result of its mutation, which classifies it also in the class of (1+1)-ES (Evolutionary Strategy). Furthermore, stohastic algorithms that do not use populations of candidate solutions and thus do not employ recombination, can also be classified in the class of Stochastic Hill Climbing algorithms :cite:`brownlee2011clever`. Further benefit of basing the |algo| algorithm on HereBoy is the simplicity of mutation procedure employed by HereBoy. HereBoy utilizes the simple technic of adaptive random search for mutations, which can be implemented efficently both regarding the time needed for execution and hardware resources needed (having embedded systems as target in mind).

The |algo| algorithm was chosen to be accelerated by hardware, since it does not use the population of individuals as most of EA-based DT algorithms do :cite:`bot2000application,krketowski2005global,llora2004mixed,papagelis2000ga`. As far as authors are aware, this is the first full DT building algorithm that operates on a single-individual population. This makes the |algo| algorithm particularly interesting to be used in embedded applications, where memory and processing resources are tightly constrained. The |algo| algorithm proved to provide smaller DTs with similar or better classification accuracy than other well-known DT inference algorithms, both incremental and full DT :cite:`vukobratovic2015evolving`.

Hardware aided decision tree induction
--------------------------------------

In order to accelerate the DT induction phase, two general approaches can be used. The first approach focuses on developing new algorithmic frameworks or new software tools, and is the dominant way of meeting this requirement :cite:`bekkerman2011scaling,choudhary2011accelerating`. The second approach focuses on the hardware acceleration of machine learning algorithms, by developing new hardware architectures optimized for accelerating the selected machine learning systems.

The hardware acceleration of the machine learning algorithms receives a significant attention in the scientific community. A wide range of solutions have been suggested in the open literature for various predictive models. The authors are aware of the work that has been done on accelerating SVMs and ANNs, where hardware architectures for the acceleration of both learning phase and the execution have been proposed. The architectures for the hardware acceleration of SVM learning algorithms have been proposed in :cite:`anguita2003digital`, while the architectures for the acceleration of previously created SVMs have been proposed in :cite:`papadonikolakis2012novel,anguita2011fpga,mahmoodi2011fpga,vranjkovic2011new`. The research in the hardware acceleration of ANNs has been particularly intensive. Numerous hardware architectures for the acceleration of already learned ANNs have been proposed :cite:`savich2012scalable,vainbrand2011scalable,echanobe2014fpga`. Also, a large number of hardware architectures capable of implementing ANN learning algorithms in hardware have been proposed :cite:`misra2010artificial,omondi2006fpga,madokoro2013hardware`. However, in the field of hardware acceleration of the DTs, the majority of the papers focus on the acceleration of already created DTs :cite:`struharik2009intellectual,li2011low,saqib2015pipelined`. Hardware acceleration of DT induction phase is scarcely covered. The authors are currently aware of only two papers on the topic of hardware acceleration of the DT induction algorithms :cite:`struharik2009evolving,chrysos2013hc`. However, both of these results focus on accelerating greedy top-down DT induction approaches. In :cite:`struharik2009evolving` the incremental DT induction algorithm, where EA is used to calculate the optimal coefficient vector one node at a time, is completely accelerated in hardware. In :cite:`chrysos2013hc` a HW/SW approach was used to accelerate the computationally most demanding part of the well known CART incremental DT induction algorithm.

In this thesis, a co-processor called |cop| (Evolutionary Full Tree Induction co-Processor) that can be used for the acceleration of the |algo| algorithm is proposed. As mentioned earlier, full DT induction algorithms typically build better DTs (smaller and more accurate) when compared to the incremental DT induction algorithms. However, full DT induction algorithms are more computationally demanding, requiring much more time to build a DT. This is one of the reasons why incremental DT induction algorithms are currently dominating the DT field. Developing a hardware accelerator for full DT induction algorithm should significantly decrease the DT inference time, and therefore make it more attractive. As far as the authors are aware, this is the first hardware accelerator in open literature concerned with the hardware acceleration of full DT induction algorithm. Being that the EAs are iterative by nature and extensively perform simple computations on the data, the |algo| algorithm should benefit from the hardware acceleration, as would any other DT induction algorithm based on the EAs. Proposed |cop| co-processor is designed to accelerate only the most computationally intensive part of the |algo| algorithm, leaving the remaining parts of the algorithm in software. It is shown later in the thesis, that the most critical part of the |algo| algorithm is the training set classification step from the fitness evaluation phase. |cop| has been designed to accelerate this step in hardware. Another advantage of this HW/SW co-design approach is that the proposed |cop| co-processor can be used with a wide variety of other EA-based DT induction algorithms :cite:`barros2012survey,bot2000application,krketowski2005global,llora2004mixed,papagelis2000ga` to accelerate the training set classification step that is always present during the fitness evaluation phase.

Induction of decision tree ensembles
------------------------------------

The ensemble classifier systems can be used to further improve the classification performance :cite:`rokach2010ensemble`. The ensemble classifier combines predictions from several individual classifiers in order to obtain a classifier that outperforms every one of them. The ensemble learning requires creation of a set of individually trained classifiers, typically DTs or ANNs, whose predictions are then combined during the process of classification of previously unseen instances. Although simple, this idea has proved to be effective, producing systems that are more accurate than a single classifier.

In the process of creation of ensemble classifiers, two problems have to be solved: ensuring the diversity of ensemble members and devising a procedure for combining individual member predictions in order to amplify correct decisions and suppress the wrong ones. Some of the most popular methods for ensuring ensemble's diversity are Breiman's bagging :cite:`buhlmann2012bagging`, Shapire's boosting :cite:`buhlmann2012bagging`, AdaBoost :cite:`buhlmann2012bagging`, Wolpert's stacked generalization :cite:`ozay2008performance`, and mixture of experts :cite:`jacobs1991adaptive`. Most commonly used combination rules include: majority voting, weighted majority voting and behavior knowledge spaces :cite:`huang1993behavior`.

The main advantage of ensemble classifier over single classifier systems is the higher accuracy and greater robustness of ensemble classifier systems. However, large amounts of memory are needed to store the ensemble classifier and high computing power is required to calculate the ensemble's output, when compared with the single classifier solutions, leading to much longer ensemble inference and instance classification times. This is because ensemble classifiers typically combine 30 or more individual classifiers :cite:`buhlmann2012bagging` so, if we want to get the same performance as with the single classifier system, 30+ times more memory and computing power would be required. Once more, hardware acceleration of ensemble classifier offers a way of achieving this goal.

Hardware aided induction of decision tree ensembles
---------------------------------------------------

Concerning the hardware acceleration of ensemble classifier systems, according to our best knowledge, most of the proposed solutions are related to the hardware implementation of ensemble classifiers that were previously inferred in the software. Most of the proposed solutions are concerned with the hardware acceleration of homogeneous ensemble classifiers :cite:`bermak2003compact,osman2009random,van2012accelerating,hussain2012adaptive,struharik2013hardware`. As far as the authors are aware, there is only one proposed solution to the hardware implementation of heterogeneous ensemble classifiers :cite:`shi2008committee`. Please notice, that all these solutions are only capable of implementing ensemble classifiers systems that were previously inferred in software, running on some general purpose processor. Authors are aware of only one paper :cite:`struharik2009evolving`, that proposes an architecture for the hardware evolution of homogeneous ensemble classifier systems based on the DTs. This solution uses the DT inference algorithm that incrementally creates DTs that are members of the ensemble classifier system.

However, in the hardware implementation the main concern is the number of required hardware resources, mainly memory, necessary to implement a DT ensemble classifier. Smaller DTs are preferred because they require less hardware resources for the implementation and lead to ensembles with the smaller hardware footprint. Therefore, algorithms for DT ensemble classifier induction that generate small, but still accurate, DTs are of great interest when the hardware implementation of DT ensemble classifiers is considered. This requirement puts the full DT induction algorithms into focus.

In this thesis, a co-processor called |ecop| (DT Ensemble Evolution co-Processor) is presented. It is shown how |ecop| can be used for hardware acceleration of |ealgo|, a full DT ensemble evolutionary induction algorithm based on Bootstrap Aggregation, also known as Bagging. The Bagging algorithm was chosen since it makes the induction of the individual ensemble members completely decoupled from each other, making it very well suited for the parallelization and hence hardware acceleration. The |ealgo| algorithm uses |algo| :cite:`vukobratovic2015evolving` (Evolutionary Full Tree Induction) algorithm that performs the induction of the full oblique classification DTs. The |algo| algorithm was chosen as the ensemble member inducer since it provides smaller DTs with similar or better classification accuracy than the other well-known DT inference algorithms, both incremental and full DTs :cite:`vukobratovic2015evolving`. However, |algo| is more computationally demanding than the incremental inducers, hence |ealgo| could merit greatly from the hardware acceleration, making it more attractive. In this paper, |ecop| co-processor is proposed to accelerate parts of the |ealgo| that are most computationally intensive, with the remaining parts of the algorithm running on the CPU. The |ecop| co-processor architecture benefits also from the fact that the |algo| algorithm evolves the DT using only one individual, in contrast to many other algorithms based on the EA that require populations :cite:`bot2000application,krketowski2005global,llora2004mixed,papagelis2000ga`. The architecture can thus be simplified with hardware resources allocated only for a single individual per ensemble member. Furthermore, by using the HW/SW co-design approach, proposed |ecop| co-processor can be used to accelerate DT ensemble inducers based on the Bagging algorithm which rely on a variety of other EA-based DT induction algorithms :cite:`barros2012survey,bot2000application,krketowski2005global,llora2004mixed,papagelis2000ga`. As far as the authors are aware, this is the first paper concerned with the hardware acceleration of full DT ensemble induction algorithm based on bagging.
